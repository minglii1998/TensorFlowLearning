{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    v = tf.get_variable(\n",
    "    \"v\",initializer=tf.zeros_initializer()(shape=[1]))\n",
    "    \n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    v = tf.get_variable(\n",
    "    \"v\",initializer=tf.ones_initializer()(shape=[1]))\n",
    "    \n",
    "# 书上的最后一行的zeros_initializer是没有加括号的，会出bug，加括号就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g1) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.variable_scope(\"\",reuse = True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g2) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    with tf.variable_scope(\"\",reuse = True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0,2.0],name=\"a\")\n",
    "b = tf.constant([2.0,3.0],name=\"b\")\n",
    "result = tf.add(a,b,name=\"add\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 5.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(result.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "x = tf.constant([[0.7,0.9]])\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(w1.initializer)\n",
    "sess.run(w2.initializer)\n",
    "\n",
    "print(sess.run(y))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "# 通过placeholder实现前向传播算法\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(1,2),name=\"input\")\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess.run(init_op)\n",
    "\n",
    "print(sess.run(y,feed_dict={x:[[0.7,0.9]]}))\n",
    "\n",
    "# 和上面的差别倒也没多大，主要就是这个place_holder，至于后面的init_op，都可以这么用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training steps,cross entropy on all data is 0.0674925\n",
      "After 1000 training steps,cross entropy on all data is 0.0163385\n",
      "After 2000 training steps,cross entropy on all data is 0.00907547\n",
      "After 3000 training steps,cross entropy on all data is 0.00714436\n",
      "After 4000 training steps,cross entropy on all data is 0.00578471\n",
      "[[-1.9618275  2.582354   1.6820377]\n",
      " [-3.4681718  1.0698231  2.11789  ]]\n",
      "[[-1.824715 ]\n",
      " [ 2.6854665]\n",
      " [ 1.418195 ]]\n"
     ]
    }
   ],
   "source": [
    "# 完整的神经网络样例 二分类\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2),name='x-input')\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1),name='y-input')\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "                y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "\n",
    "Y = [[int(x1+x2 < 1)] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    STEPS = 5000\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        \n",
    "        start = (i*batch_size)%dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        \n",
    "        sess.run(train_step,\n",
    "                feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        \n",
    "        if i % 1000 ==0:\n",
    "            total_cross_entropy = sess.run(\n",
    "                cross_entropy,feed_dict={x:X,y_:Y})\n",
    "            print('After %d training steps,cross entropy on all data is %g'\n",
    "                 % (i,total_cross_entropy))\n",
    "\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "所有的神经网络训练都可以分为3个步骤：\n",
    "1. 定义神经网络的结构和前向传播的输出算法\n",
    "2. 定义损失函数以及选择反向传播算法\n",
    "3. 生成会话并且在训练数据上反复运行反向传播优化算法\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
